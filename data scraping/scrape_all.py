# -*- coding: utf-8 -*-
"""
scrape_all.py
ÙŠØ¬Ù…Ø¹ ÙƒÙ„ Ø§Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…Ù† Ù…ÙˆÙ‚Ø¹ Ù‡ÙŠØ¦Ø© Ø§Ù„Ø®Ø¨Ø±Ø§Ø¡
ÙˆÙŠØ­ÙØ¸ ÙƒÙ„ Ù†Ø¸Ø§Ù… ÙÙŠ Ù…Ù„Ù JSON Ø¯Ø§Ø®Ù„ data/boe_laws_json
"""

import os
import time
import json
import datetime
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from slugify import slugify
import urllib3

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

BASE_URL = "https://laws.boe.gov.sa"
HEADERS = {"User-Agent": "LegalScraper/0.2 (for research use)"}

# Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø­ÙØ¸
BASE_DIR = os.path.dirname(os.path.dirname(__file__))
OUT_DIR = os.path.join(BASE_DIR, "data", "boe_laws_json")
os.makedirs(OUT_DIR, exist_ok=True)


def get_soup(url):
    """Ø¬Ù„Ø¨ Ø§Ù„ØµÙØ­Ø© ÙˆØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¥Ù„Ù‰ BeautifulSoup"""
    r = requests.get(url, headers=HEADERS, timeout=25, verify=False)
    r.raise_for_status()
    r.encoding = r.apparent_encoding or "utf-8"
    return BeautifulSoup(r.text, "lxml")


def list_laws_from_folder(folder_id):
    """Ø¥Ø±Ø¬Ø§Ø¹ Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø±ÙˆØ§Ø¨Ø· ÙƒÙ„ Ø§Ù„Ø£Ù†Ø¸Ù…Ø© Ø¯Ø§Ø®Ù„ ØªØµÙ†ÙŠÙ Ù…Ø¹ÙŠÙ†"""
    folder_url = f"{BASE_URL}/BoeLaws/Laws/Folders/{folder_id}"
    soup = get_soup(folder_url)
    links = []
    for a in soup.select("a[href*='/BoeLaws/Laws/LawDetails/']"):
        href = a.get("href")
        if href:
            links.append(urljoin(BASE_URL, href))
    return sorted(set(links))


def parse_articles(soup):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†ØµÙˆØµ Ø§Ù„Ù…ÙˆØ§Ø¯ Ù…Ù† ØµÙØ­Ø© Ø§Ù„Ù†Ø¸Ø§Ù…"""
    text = soup.get_text("\n", strip=True)
    import re
    parts = re.split(r"(?=Ø§Ù„Ù…Ø§Ø¯Ø©\s+\d+|Ù…Ø§Ø¯Ø©\s+\d+)", text)
    articles = []
    for p in parts:
        p = p.strip()
        if not p:
            continue
        m = re.match(r"(?:Ø§Ù„Ù…Ø§Ø¯Ø©|Ù…Ø§Ø¯Ø©)\s+(\d+)", p)
        num = int(m.group(1)) if m else None
        articles.append({
            "number": f"Ø§Ù„Ù…Ø§Ø¯Ø© {num}" if num else None,
            "number_norm": num,
            "text": p
        })
    return articles


def scrape_one_law(url):
    """ÙŠØ¬Ù…Ø¹ Ù†Ø¸Ø§Ù… ÙˆØ§Ø­Ø¯ ÙˆÙŠØ­ÙØ¸Ù‡ ÙƒÙ…Ù„Ù JSON"""
    soup = get_soup(url)
    h1 = soup.select_one("h1")
    title = h1.get_text(strip=True) if h1 else "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†"
    articles = parse_articles(soup)

    data = {
        "title_ar": title,
        "source_url": url,
        "articles": articles,
        "scraped_at": datetime.datetime.utcnow().isoformat() + "Z"
    }

    slug = slugify(title)
    out_path = os.path.join(OUT_DIR, f"{slug}.json")
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"âœ… Saved: {title} ({len(articles)} articles)")
    return out_path


def main():
    # Ù‚Ø§Ø¦Ù…Ø© ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„Ø£Ù†Ø¸Ù…Ø© ÙÙŠ Ø§Ù„Ù…ÙˆÙ‚Ø¹ ()
    folder_ids = [1, 2, 3, 4, 5]
    all_links = set()

    print("ğŸ” Gathering law links...")
    for fid in folder_ids:
        try:
            urls = list_laws_from_folder(fid)
            print(f"ğŸ“‚ Folder {fid}: {len(urls)} links")
            all_links.update(urls)
            time.sleep(1.5)
        except Exception as e:
            print("âš ï¸ Error reading folder", fid, e)

    print(f"\nğŸ“š Total unique laws found: {len(all_links)}\n")

    for url in sorted(all_links):
        try:
            scrape_one_law(url)
        except Exception as e:
            print("âŒ Error scraping:", url, e)
        time.sleep(2)  # ØªØ£Ø®ÙŠØ± Ø¨Ø³ÙŠØ· Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø¶ØºØ· Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆÙ‚Ø¹


if __name__ == "__main__":
    main()
